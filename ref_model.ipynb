{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "12SvoeBjdeuZncWgMeLTyaopVNb59jwZ9",
      "authorship_tag": "ABX9TyOLKsEpgOxJIeHXgs54G4nH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skyshine460/GHG_Direct_Emissions/blob/main/ref_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pygeohash"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAAvY8SydMBB",
        "outputId": "c92e75c8-f705-4344-f083-cec215fd153e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pygeohash\n",
            "  Downloading pygeohash-1.2.0.tar.gz (5.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pygeohash\n",
            "  Building wheel for pygeohash (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pygeohash: filename=pygeohash-1.2.0-py2.py3-none-any.whl size=6153 sha256=4e1cb978bb724216893100d48fcf1f929447c024e180b4c82e43a3eb0ee0dda3\n",
            "  Stored in directory: /root/.cache/pip/wheels/28/ec/b6/beadf7295a623f528507691fb0d471b50d064ae9bbad420b8f\n",
            "Successfully built pygeohash\n",
            "Installing collected packages: pygeohash\n",
            "Successfully installed pygeohash-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZCfJUYDdTpc",
        "outputId": "653e3984-4f9d-49bc-d015-d5e5133288ed"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.13.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.2.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (9.0.0)\n",
            "Downloading catboost-1.2.7-cp310-cp310-manylinux2014_x86_64.whl (98.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1fkDm39xbLdm"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import joblib\n",
        "import math\n",
        "\n",
        "import pygeohash as pgh\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import *\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mj0rnGI7dBPO",
        "outputId": "5d2c3693-91a6-4282-fa91-3960f9bf16b8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 予測モデルを訓練するためのデータセット\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/GHGの排出量の予測にチャレンジしよう！（SMBC Group GREEN×DATA Challenge 2024）/提供データ/train.csv', index_col=0)\n",
        "\n",
        "# 予測モデルに推論（予測)させるデータセット\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/GHGの排出量の予測にチャレンジしよう！（SMBC Group GREEN×DATA Challenge 2024）/提供データ/test.csv', index_col=0)"
      ],
      "metadata": {
        "id": "3yD9DfwbcwFe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# del train_df['Unnamed: 0'], test_df['Unnamed: 0']\n",
        "del train_df['FacilityName'], test_df['FacilityName']\n",
        "del train_df['LocationAddress'], test_df['LocationAddress']\n",
        "del train_df['ZIP'], test_df['ZIP']\n",
        "del train_df['IndustryType'], test_df['IndustryType']\n",
        "del train_df['SecondPrimaryNAICS'], test_df['SecondPrimaryNAICS']"
      ],
      "metadata": {
        "id": "aepPvmt4bP6k"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "two_digit_map     = {11: 'Agriculture, Forestry, Fishing and Hunting',\n",
        "                    21: 'Mining, Quarrying, and Oil and Gas Extraction',\n",
        "                    22: 'Utilities',\n",
        "                    23: 'Construction',\n",
        "                    31: 'Manufacturing',\n",
        "                    32: 'Manufacturing',\n",
        "                    33: 'Manufacturing',\n",
        "                    42: 'Wholesale Trade',\n",
        "                    44: 'Retail Trade',\n",
        "                    45: 'Retail Trade',\n",
        "                    48: 'Transportation and Warehousing',\n",
        "                    49: 'Transportation and Warehousing',\n",
        "                    51: 'Information',\n",
        "                    52: 'Finance and Insurance',\n",
        "                    53: 'Real Estate and Rental and Leasing',\n",
        "                    54: 'Professional, Scientific, and Technical Services',\n",
        "                    55: 'Management of Companies and Enterprises',\n",
        "                    56: 'Administrative and Support and Waste Management and Remediation Services',\n",
        "                    61: 'Educational Services',\n",
        "                    62: 'Health Care and Social Assistance',\n",
        "                    71: 'Arts, Entertainment, and Recreation',\n",
        "                    72: 'Accommodation and Food Services',\n",
        "                    81: 'Other Services (except Public Administration)',\n",
        "                    92: 'Public Administration'}"
      ],
      "metadata": {
        "id": "-KsnfJvKbTE4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['first_two_digit_primary_naics'] = train_df['PrimaryNAICS'].apply(lambda z: str(z)[:2]).astype(int)\n",
        "test_df['first_two_digit_primary_naics']  = test_df['PrimaryNAICS'].apply(lambda z: str(z)[:2]).astype(int)\n",
        "\n",
        "train_df['Economic_Sector']               = train_df['first_two_digit_primary_naics'].map(two_digit_map)\n",
        "test_df['Economic_Sector']                = test_df['first_two_digit_primary_naics'].map(two_digit_map)\n",
        "\n",
        "del train_df['first_two_digit_primary_naics'], test_df['first_two_digit_primary_naics']\n",
        "\n",
        "econ_sector_train                         = train_df['Economic_Sector'].values\n",
        "econ_sector_test                          = test_df['Economic_Sector'].values"
      ],
      "metadata": {
        "id": "CKG9fp8PbWJn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_distance = pd.read_csv('/content/drive/MyDrive/GHGの排出量の予測にチャレンジしよう！（SMBC Group GREEN×DATA Challenge 2024）/特徴量/train_distance_5nbrs_.csv')\n",
        "test_distance = pd.read_csv('/content/drive/MyDrive/GHGの排出量の予測にチャレンジしよう！（SMBC Group GREEN×DATA Challenge 2024）/特徴量/test_distance_5nbrs_.csv')"
      ],
      "metadata": {
        "id": "_hLn0b4M9N76"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def haversine(lat1, lon1, lat2, lon2):\n",
        "#     # Convert latitude and longitude from degrees to radians\n",
        "#     lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
        "\n",
        "#     # Haversine formula\n",
        "#     dlat = lat2 - lat1\n",
        "#     dlon = lon2 - lon1\n",
        "#     a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n",
        "#     c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
        "\n",
        "#     # Radius of Earth in kilometers (mean radius)\n",
        "#     R = 6371.0\n",
        "\n",
        "#     # Calculate the distance\n",
        "#     distance = R * c\n",
        "#     return distance\n"
      ],
      "metadata": {
        "id": "mB1IM76_bb0p"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def calculate_distance(lat1, lon1, lat2, lon2):\n",
        "#     return haversine(lat1, lon1, lat2, lon2)\n",
        "\n",
        "# def get_nearest_distance(lat1, lon1, econ_sector_ref, neighbours=5, train_point=False):\n",
        "#     # Extract the required columns once to NumPy arrays\n",
        "#     latitudes = train_df['Latitude'].values\n",
        "#     longitudes = train_df['Longitude'].values\n",
        "#     ghg_emissions = train_df['GHG_Direct_Emissions_14_in_metric_tons'].values\n",
        "#     econ_sectors = train_df['Economic_Sector'].values\n",
        "\n",
        "#     # Filter NaN emissions directly\n",
        "#     valid_indices = ~np.isnan(ghg_emissions)\n",
        "\n",
        "#     latitudes = latitudes[valid_indices]\n",
        "#     longitudes = longitudes[valid_indices]\n",
        "#     ghg_emissions = ghg_emissions[valid_indices]\n",
        "#     econ_sectors = econ_sectors[valid_indices]\n",
        "\n",
        "#     # Parallelize distance calculation using ThreadPoolExecutor\n",
        "#     with ThreadPoolExecutor() as executor:\n",
        "#         distances = list(executor.map(calculate_distance,\n",
        "#                                      [lat1]*len(latitudes),\n",
        "#                                      [lon1]*len(longitudes),\n",
        "#                                      latitudes,\n",
        "#                                      longitudes))\n",
        "\n",
        "#     # Combine the results into a DataFrame\n",
        "#     near_df = pd.DataFrame({\n",
        "#         'Distance': distances,\n",
        "#         'GHG_emission_14': ghg_emissions,\n",
        "#         'Economic_Sector': econ_sectors\n",
        "#     })\n",
        "\n",
        "\n",
        "#     if train_point:\n",
        "#         near_df.sort_values(by='Distance', inplace=True)\n",
        "#         near_df = near_df.dropna()\n",
        "#         near_df = near_df.iloc[1:].reset_index(drop=True)\n",
        "#     else:\n",
        "#         # Sort distances by 'Distance'\n",
        "#         near_df.sort_values(by='Distance', inplace=True)\n",
        "#         near_df = near_df.dropna()\n",
        "\n",
        "#     # Filter by economic sector\n",
        "#     nearest_locations_econ_sector = near_df[near_df['Economic_Sector'] == econ_sector_ref]\n",
        "\n",
        "#     # Get the top N nearest for both economic sector and overall\n",
        "#     sub_near_econ = nearest_locations_econ_sector.head(neighbours)\n",
        "#     sub_nearest_locations = near_df.head(neighbours)\n",
        "\n",
        "#     # Compute weighted averages\n",
        "#     econ_weighted_average = (sub_near_econ['GHG_emission_14'] / np.where(sub_near_econ['Distance'] == 0, 1, sub_near_econ['Distance']) ).sum()\n",
        "#     near_weighted_average = (sub_nearest_locations['GHG_emission_14'] / np.where(sub_nearest_locations['Distance']==0,1,sub_nearest_locations['Distance'])).sum()\n",
        "\n",
        "#     # Compute regular averages\n",
        "#     econ_average = sub_near_econ['GHG_emission_14'].mean()\n",
        "#     near_average = sub_nearest_locations['GHG_emission_14'].mean()\n",
        "\n",
        "#     return [econ_weighted_average, econ_average, near_weighted_average, near_average]"
      ],
      "metadata": {
        "id": "gKmXQV4objgW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# neighbours = 5\n",
        "\n",
        "# file_path = f\"test_distance_{neighbours}nbrs.csv\"\n",
        "# if os.path.exists(file_path):\n",
        "#     print(f\"{file_path} exists. \")\n",
        "#     test_distance = pd.read_csv(f\"test_distance_{neighbours}nbrs.csv\")\n",
        "# else:\n",
        "#     output = []\n",
        "#     for index in tqdm(range(test_df.shape[0])):\n",
        "\n",
        "#         lat1 = test_df.iloc[index]['Latitude']\n",
        "#         lon1 = test_df.iloc[index]['Longitude']\n",
        "#         econ_sector_ref = test_df.iloc[index]['Economic_Sector']\n",
        "#         x = get_nearest_distance(lat1,lon1,econ_sector_ref,neighbours=neighbours,train_point=False)\n",
        "#         output.append(x)\n",
        "\n",
        "#     test_distance = pd.DataFrame(output,columns = ['Economy_Sector_Weighted_Avg','Economic_Sector_Average','Nearest_Weighted_Average','Nearest_Average'])\n",
        "\n",
        "#     test_distance.to_csv(f'test_distance_{neighbours}nbrs.csv',index=False)\n",
        "\n",
        "\n",
        "# file_path = f\"train_distance_{neighbours}nbrs.csv\"\n",
        "# if os.path.exists(file_path):\n",
        "#     print(f\"{file_path} exists. \")\n",
        "#     train_distance = pd.read_csv(f'train_distance_{neighbours}nbrs.csv')\n",
        "# else:\n",
        "#     output = []\n",
        "#     for index in tqdm(range(train_df.shape[0])):\n",
        "#         lat1 = train_df.iloc[index]['Latitude']\n",
        "#         lon1 = train_df.iloc[index]['Longitude']\n",
        "#         econ_sector_ref = train_df.iloc[index]['Economic_Sector']\n",
        "#         x = get_nearest_distance(lat1,lon1,econ_sector_ref,neighbours=neighbours, train_point=True)\n",
        "#         output.append(x)\n",
        "\n",
        "#     train_distance = pd.DataFrame(output,columns = ['Economy_Sector_Weighted_Avg','Economic_Sector_Average','Nearest_Weighted_Average','Nearest_Average'])\n",
        "#     train_distance.to_csv(f'train_distance_{neighbours}nbrs.csv',index=False)\n",
        "\n",
        "\n",
        "\n",
        "train_df = pd.concat((train_df,train_distance),axis=1)\n",
        "test_df  = pd.concat((test_df,test_distance),axis=1)"
      ],
      "metadata": {
        "id": "fIiv6NWhbprU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_features1(df):\n",
        "\n",
        "\n",
        "    # Year-over-year difference\n",
        "    df['TRI_Air_Emissions_YoY_Change_11'] = df['TRI_Air_Emissions_11_in_lbs'] - df['TRI_Air_Emissions_10_in_lbs']\n",
        "    df['TRI_Air_Emissions_YoY_Change_12'] = df['TRI_Air_Emissions_12_in_lbs'] - df['TRI_Air_Emissions_11_in_lbs']\n",
        "    df['TRI_Air_Emissions_YoY_Change_13'] = df['TRI_Air_Emissions_13_in_lbs'] - df['TRI_Air_Emissions_12_in_lbs']\n",
        "\n",
        "   # Year-over-year growth rate with NaN protection\n",
        "    df['TRI_Air_Emissions_Growth_Rate_11'] = np.where(\n",
        "        df['TRI_Air_Emissions_10_in_lbs'].notna() & (df['TRI_Air_Emissions_10_in_lbs'] != 0),\n",
        "        (df['TRI_Air_Emissions_11_in_lbs'] - df['TRI_Air_Emissions_10_in_lbs']) / df['TRI_Air_Emissions_10_in_lbs'],\n",
        "        np.nan\n",
        "    )\n",
        "\n",
        "    df['TRI_Air_Emissions_Growth_Rate_12'] = np.where(\n",
        "        df['TRI_Air_Emissions_11_in_lbs'].notna() & (df['TRI_Air_Emissions_11_in_lbs'] != 0),\n",
        "        (df['TRI_Air_Emissions_12_in_lbs'] - df['TRI_Air_Emissions_11_in_lbs']) / df['TRI_Air_Emissions_11_in_lbs'],\n",
        "        np.nan\n",
        "    )\n",
        "\n",
        "    df['TRI_Air_Emissions_Growth_Rate_13'] = np.where(\n",
        "        df['TRI_Air_Emissions_12_in_lbs'].notna() & (df['TRI_Air_Emissions_12_in_lbs'] != 0),\n",
        "        (df['TRI_Air_Emissions_13_in_lbs'] - df['TRI_Air_Emissions_12_in_lbs']) / df['TRI_Air_Emissions_12_in_lbs'],\n",
        "        np.nan\n",
        "    )\n",
        "\n",
        "    return df\n",
        "\n",
        "train_df      = create_features1(train_df)\n",
        "test_df       = create_features1(test_df)\n",
        "new_features1 = ['TRI_Air_Emissions_YoY_Change_11','TRI_Air_Emissions_YoY_Change_12','TRI_Air_Emissions_YoY_Change_13',\n",
        "                 'TRI_Air_Emissions_Growth_Rate_11','TRI_Air_Emissions_Growth_Rate_12','TRI_Air_Emissions_Growth_Rate_13']\n"
      ],
      "metadata": {
        "id": "DuD0maJEb1i5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_features2(df):\n",
        "\n",
        "\n",
        "    # Year-over-year difference\n",
        "    df['GHG_Direct_Emissions_YoY_Change_11'] = df['GHG_Direct_Emissions_11_in_metric_tons'] - df['GHG_Direct_Emissions_10_in_metric_tons']\n",
        "    df['GHG_Direct_Emissions_YoY_Change_12'] = df['GHG_Direct_Emissions_12_in_metric_tons'] - df['GHG_Direct_Emissions_11_in_metric_tons']\n",
        "    df['GHG_Direct_Emissions_YoY_Change_13'] = df['GHG_Direct_Emissions_13_in_metric_tons'] - df['GHG_Direct_Emissions_12_in_metric_tons']\n",
        "\n",
        "    # Year-over-year growth rate for GHG Direct Emissions with NaN protection\n",
        "    df['GHG_Direct_Emissions_Growth_Rate_11'] = np.where(\n",
        "        df['GHG_Direct_Emissions_10_in_metric_tons'].notna() & (df['GHG_Direct_Emissions_10_in_metric_tons'] != 0),\n",
        "        (df['GHG_Direct_Emissions_11_in_metric_tons'] - df['GHG_Direct_Emissions_10_in_metric_tons']) / df['GHG_Direct_Emissions_10_in_metric_tons'],\n",
        "        np.nan\n",
        "    )\n",
        "\n",
        "    df['GHG_Direct_Emissions_Growth_Rate_12'] = np.where(\n",
        "        df['GHG_Direct_Emissions_11_in_metric_tons'].notna() & (df['GHG_Direct_Emissions_11_in_metric_tons'] != 0),\n",
        "        (df['GHG_Direct_Emissions_12_in_metric_tons'] - df['GHG_Direct_Emissions_11_in_metric_tons']) / df['GHG_Direct_Emissions_11_in_metric_tons'],\n",
        "        np.nan\n",
        "    )\n",
        "\n",
        "    df['GHG_Direct_Emissions_Growth_Rate_13'] = np.where(\n",
        "        df['GHG_Direct_Emissions_12_in_metric_tons'].notna() & (df['GHG_Direct_Emissions_12_in_metric_tons'] != 0),\n",
        "        (df['GHG_Direct_Emissions_13_in_metric_tons'] - df['GHG_Direct_Emissions_12_in_metric_tons']) / df['GHG_Direct_Emissions_12_in_metric_tons'],\n",
        "        np.nan\n",
        "    )\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "new_features2 = ['GHG_Direct_Emissions_YoY_Change_11','GHG_Direct_Emissions_YoY_Change_12','GHG_Direct_Emissions_YoY_Change_13',\n",
        "                 'GHG_Direct_Emissions_Growth_Rate_11','GHG_Direct_Emissions_Growth_Rate_12','GHG_Direct_Emissions_Growth_Rate_13'\n",
        "                 ]\n",
        "train_df      = create_features2(train_df)\n",
        "test_df       = create_features2(test_df)"
      ],
      "metadata": {
        "id": "2zv0CSwKb7iK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_features3(df):\n",
        "    df['TRI_to_GHG_Ratio_10'] = np.where(df['GHG_Direct_Emissions_10_in_metric_tons'].notna(),\n",
        "                                     df['TRI_Air_Emissions_10_in_lbs'] / df['GHG_Direct_Emissions_10_in_metric_tons'],\n",
        "                                     np.nan)\n",
        "\n",
        "    df['TRI_to_GHG_Ratio_11'] = np.where(df['GHG_Direct_Emissions_11_in_metric_tons'].notna(),\n",
        "                                        df['TRI_Air_Emissions_11_in_lbs'] / df['GHG_Direct_Emissions_11_in_metric_tons'],\n",
        "                                        np.nan)\n",
        "\n",
        "    df['TRI_to_GHG_Ratio_12'] = np.where(df['GHG_Direct_Emissions_12_in_metric_tons'].notna(),\n",
        "                                        df['TRI_Air_Emissions_12_in_lbs'] / df['GHG_Direct_Emissions_12_in_metric_tons'],\n",
        "                                        np.nan)\n",
        "\n",
        "    df['TRI_to_GHG_Ratio_13'] = np.where(df['GHG_Direct_Emissions_13_in_metric_tons'].notna(),\n",
        "                                        df['TRI_Air_Emissions_13_in_lbs'] / df['GHG_Direct_Emissions_13_in_metric_tons'],\n",
        "                                        np.nan)\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "train_df      = create_features3(train_df)\n",
        "test_df       = create_features3(test_df)\n",
        "new_features3 = ['TRI_to_GHG_Ratio_10','TRI_to_GHG_Ratio_11','TRI_to_GHG_Ratio_12','TRI_to_GHG_Ratio_13']"
      ],
      "metadata": {
        "id": "c8zMly0JcABw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_df = train_df.groupby(['Economic_Sector', 'State']).agg({'GHG_Direct_Emissions_14_in_metric_tons': ['mean', 'median', 'max', 'min','count']})\n",
        "\n",
        "summary_df.columns = [\n",
        "                        'GHG_Direct_Emissions_14_in_metric_tons_mean',\n",
        "                        'GHG_Direct_Emissions_14_in_metric_tons_median',\n",
        "                        'GHG_Direct_Emissions_14_in_metric_tons_max',\n",
        "                        'GHG_Direct_Emissions_14_in_metric_tons_min',\n",
        "                        'GHG_Direct_Emissions_14_in_metric_tons_count'\n",
        "                     ]\n",
        "\n",
        "summary_df = summary_df.reset_index()\n",
        "\n",
        "train_df = train_df.merge(summary_df, on=['Economic_Sector', 'State'], how='left')\n",
        "test_df  = test_df.merge(summary_df,  on=['Economic_Sector', 'State'], how='left')\n"
      ],
      "metadata": {
        "id": "QheQjnL_cEv3"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = pd.concat((train_df,test_df),axis=0)\n",
        "for cols in ['City','State','County','FIPScode','PrimaryNAICS','Economic_Sector']:\n",
        "    le              = LabelEncoder()\n",
        "    merged_df[cols] = le.fit_transform(merged_df[cols].values.reshape(-1,1))\n",
        "\n",
        "train_df = merged_df.iloc[:train_df.shape[0],:]\n",
        "test_df  = merged_df.iloc[train_df.shape[0]:,:]\n",
        "train_df.shape,test_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcxLwwNvcHnD",
        "outputId": "bdbe9cce-c983-48d5-fd4c-afd29806df99"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4655, 42), (5016, 42))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_columns = [\n",
        "                    'TRI_Air_Emissions_10_in_lbs', 'TRI_Air_Emissions_11_in_lbs',\n",
        "                    'TRI_Air_Emissions_12_in_lbs', 'TRI_Air_Emissions_13_in_lbs','PrimaryNAICS',\n",
        "                    'GHG_Direct_Emissions_10_in_metric_tons', 'GHG_Direct_Emissions_11_in_metric_tons',\n",
        "                    'GHG_Direct_Emissions_12_in_metric_tons', 'GHG_Direct_Emissions_13_in_metric_tons',\n",
        "                    ]\n",
        "lat_lon_columns   = ['Latitude','Longitude']\n",
        "target_columns    = ['GHG_Direct_Emissions_14_in_metric_tons']\n",
        "categorical_columns = ['City','State','County','FIPScode','PrimaryNAICS','Economic_Sector']\n",
        "train_aggregations  =  [\n",
        "                        'GHG_Direct_Emissions_14_in_metric_tons_mean',\n",
        "                        'GHG_Direct_Emissions_14_in_metric_tons_median',\n",
        "                        'GHG_Direct_Emissions_14_in_metric_tons_max',\n",
        "                        'GHG_Direct_Emissions_14_in_metric_tons_min',\n",
        "                        'GHG_Direct_Emissions_14_in_metric_tons_count'\n",
        "                     ]\n",
        "new_features1  = ['TRI_Air_Emissions_YoY_Change_11','TRI_Air_Emissions_YoY_Change_12','TRI_Air_Emissions_YoY_Change_13',\n",
        "                    'TRI_Air_Emissions_Growth_Rate_11','TRI_Air_Emissions_Growth_Rate_12','TRI_Air_Emissions_Growth_Rate_13']\n",
        "\n",
        "new_features2   = ['GHG_Direct_Emissions_YoY_Change_11','GHG_Direct_Emissions_YoY_Change_12','GHG_Direct_Emissions_YoY_Change_13',\n",
        "                  'GHG_Direct_Emissions_Growth_Rate_11','GHG_Direct_Emissions_Growth_Rate_12','GHG_Direct_Emissions_Growth_Rate_13'\n",
        "                  ]\n",
        "new_features3   = ['TRI_to_GHG_Ratio_10','TRI_to_GHG_Ratio_11','TRI_to_GHG_Ratio_12','TRI_to_GHG_Ratio_13']\n",
        "neighbour_feats = ['Economy_Sector_Weighted_Avg','Economic_Sector_Average','Nearest_Weighted_Average','Nearest_Average']\n",
        "\n",
        "\n",
        "train = train_df[numerical_columns+\n",
        "                 lat_lon_columns+\n",
        "                #  categorical_columns+\n",
        "                 new_features1+['Economic_Sector']\n",
        "                #  new_features2\n",
        "               #   new_features3+\n",
        "                #  train_aggregations+\n",
        "               #   neighbour_feats\n",
        "                 ].values\n",
        "test  = test_df[numerical_columns+\n",
        "                 lat_lon_columns+\n",
        "                #  categorical_columns+\n",
        "                 new_features1+['Economic_Sector']\n",
        "                #  new_features2\n",
        "               #   new_features3+\n",
        "                #  train_aggregations+\n",
        "               #   neighbour_feats\n",
        "                 ].values\n",
        "target = train_df[target_columns].values"
      ],
      "metadata": {
        "id": "oqUiXtVlcLwh"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_models_trained(train,test,target, num_folds):\n",
        "#     kf               = KFold(n_splits=num_folds, shuffle=True, random_state=13)\n",
        "\n",
        "#     oof_predictions  = np.zeros(len(train))\n",
        "#     test_predictions = np.zeros(len(test))\n",
        "\n",
        "\n",
        "#     for fold, (train_index, valid_index) in enumerate(kf.split(train,target)):\n",
        "\n",
        "#         X_train, X_valid             = train[train_index], train[valid_index]\n",
        "#         y_train, y_valid             = target[train_index], target[valid_index]\n",
        "\n",
        "\n",
        "#         model_dict                   = {}\n",
        "#         loss_dict                    = {}\n",
        "#         global valid_pred_dict\n",
        "#         valid_pred_dict              = {}\n",
        "\n",
        "#         params                       = {\"n_estimators\": 626, \"max_depth\": 3, \"random_state\":13,\n",
        "#                                         \"min_child_weight\": 0.001190123543553736, \"learning_rate\": 0.010519736270936835,\n",
        "#                                         \"subsample\": 0.7304788478701394, \"colsample_bylevel\": 0.604447278915981,\n",
        "#                                           \"colsample_bytree\": 0.7616852136157319, \"reg_alpha\": 0.115175569924065, \"reg_lambda\": 0.07155347824929895}\n",
        "#         model1                       = XGBRegressor(**params)\n",
        "\n",
        "#         params                       = {\"n_estimators\": 206, \"max_depth\": 3, \"min_child_weight\": 0.002124665025111174, \"random_state\":13,\n",
        "#                                         \"learning_rate\": 0.02528390491004826, \"subsample\": 0.800800019181945,\n",
        "#                                         \"colsample_bylevel\": 0.7264139639244361, \"colsample_bytree\": 0.7283285945816331,\n",
        "#                                           \"reg_alpha\": 0.13673920290025274, \"reg_lambda\": 0.008614256283329808}\n",
        "#         model2                       = XGBRegressor(**params)\n",
        "\n",
        "#         params                       = {\"n_estimators\": 56,\"random_state\":13,\n",
        "#                                          \"verbose\":-1,}\n",
        "#         model3                       = LGBMRegressor(**params)\n",
        "\n",
        "\n",
        "\n",
        "#         _                             = model1.fit(X_train,np.log1p(y_train))\n",
        "#         valid_preds1                  = np.expm1(model1.predict(X_valid))\n",
        "#         rmsle1                        = root_mean_squared_log_error(y_valid, valid_preds1)\n",
        "#         print(f\"Fold {fold+1} RMSLE for model1 = {rmsle1}\")\n",
        "\n",
        "#         _                             = model2.fit(X_train,np.log1p(y_train))\n",
        "#         valid_preds2                  = np.expm1(model2.predict(X_valid))\n",
        "#         rmsle2                        = root_mean_squared_log_error(y_valid, valid_preds2)\n",
        "#         print(f\"Fold {fold+1} RMSLE for model2 = {rmsle2}\")\n",
        "\n",
        "#         _                             = model3.fit(X_train,np.log1p(y_train))\n",
        "#         valid_preds3                  = np.expm1(model3.predict(X_valid))\n",
        "#         rmsle3                        = root_mean_squared_log_error(y_valid, valid_preds3)\n",
        "#         print(f\"Fold {fold+1} RMSLE for model3 = {rmsle3}\")\n",
        "\n",
        "\n",
        "\n",
        "#         loss_dict['model1']           = rmsle1\n",
        "#         loss_dict['model2']           = rmsle2\n",
        "#         loss_dict['model3']           = rmsle3\n",
        "\n",
        "\n",
        "#         model_dict['model1']          = model1\n",
        "#         model_dict['model2']          = model2\n",
        "#         model_dict['model3']          = model3\n",
        "\n",
        "\n",
        "#         valid_pred_dict['model1']     = valid_preds1\n",
        "#         valid_pred_dict['model2']     = valid_preds2\n",
        "#         valid_pred_dict['model3']     = valid_preds3\n",
        "\n",
        "\n",
        "\n",
        "#         valid_preds_mean              = np.mean(list(valid_pred_dict.values()), axis=0)\n",
        "#         rmsle_mean                    = root_mean_squared_log_error(y_valid, valid_preds_mean)\n",
        "#         print(f\"Fold {fold+1} Average RMSLE = {rmsle_mean}\")\n",
        "\n",
        "\n",
        "#         min_loss_model                = min(loss_dict, key=loss_dict.get)\n",
        "\n",
        "#         model                         = model_dict[min_loss_model]\n",
        "#         valid_preds_best_model        = np.expm1(model.predict(X_valid))\n",
        "#         rmsle_best                    = root_mean_squared_log_error(y_valid, valid_preds_best_model)\n",
        "\n",
        "\n",
        "#         if rmsle_mean >rmsle_best:\n",
        "#             print(f\"The average RMSLE is {rmsle_mean} while the best RMSLE is {rmsle_best} and we proceed with the model with best RMSLE\")\n",
        "#             model = model_dict[min_loss_model]\n",
        "#             oof_predictions[valid_index]  = valid_preds_best_model\n",
        "#             test_preds                    = model.predict(test)\n",
        "#             test_predictions += (test_preds) / kf.n_splits\n",
        "#             oof_predictions[valid_index]  = valid_preds_best_model\n",
        "\n",
        "\n",
        "#         else:\n",
        "#             print(f\"The average RMSLE is {rmsle_mean} while the best RMSLE is {rmsle_best} and we proceed with averaging of all models\")\n",
        "#             output_predictions = []\n",
        "#             for key,values in model_dict.items():\n",
        "#                 model       = values\n",
        "#                 test_preds  = model.predict(test)\n",
        "#                 _           = output_predictions.append(test_preds)\n",
        "#             output_preds    = np.mean(output_predictions, axis=0)\n",
        "#             test_predictions += output_preds/kf.n_splits\n",
        "#             oof_predictions[valid_index]  = valid_preds_mean\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#         gc.collect()\n",
        "\n",
        "#         print('---------------\\n')\n",
        "\n",
        "#     RMSLE = root_mean_squared_log_error(target, oof_predictions)\n",
        "#     print(f\"OOF RMSLE = {RMSLE}\")\n",
        "\n",
        "#     return oof_predictions,np.expm1(test_predictions)\n"
      ],
      "metadata": {
        "id": "NWRcxoFvcSmV"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_models_trained(train, test, target, num_folds):\n",
        "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=13)\n",
        "\n",
        "    oof_predictions = np.zeros(len(train))\n",
        "    test_predictions = np.zeros(len(test))\n",
        "    train_predictions = np.zeros(len(train))  # Trainデータ予測の格納用\n",
        "\n",
        "    for fold, (train_index, valid_index) in enumerate(kf.split(train, target)):\n",
        "        print(f\"------------------------------ fold {fold+1} ------------------------------\")\n",
        "\n",
        "        X_train, X_valid = train[train_index], train[valid_index]\n",
        "        y_train, y_valid = target[train_index], target[valid_index]\n",
        "\n",
        "        model_dict = {}\n",
        "        loss_dict = {}\n",
        "        valid_pred_dict = {}\n",
        "\n",
        "        # モデル1\n",
        "        params = {\n",
        "            \"n_estimators\": 626,\n",
        "            \"max_depth\": 3,\n",
        "            \"random_state\": 13,\n",
        "            \"min_child_weight\": 0.001190123543553736,\n",
        "            \"learning_rate\": 0.010519736270936835,\n",
        "            \"subsample\": 0.7304788478701394,\n",
        "            \"colsample_bylevel\": 0.604447278915981,\n",
        "            \"colsample_bytree\": 0.7616852136157319,\n",
        "            \"reg_alpha\": 0.115175569924065,\n",
        "            \"reg_lambda\": 0.07155347824929895,\n",
        "        }\n",
        "        model1 = XGBRegressor(**params)\n",
        "\n",
        "        # モデル2\n",
        "        params = {\n",
        "            \"n_estimators\": 206,\n",
        "            \"max_depth\": 3,\n",
        "            \"min_child_weight\": 0.002124665025111174,\n",
        "            \"random_state\": 13,\n",
        "            \"learning_rate\": 0.02528390491004826,\n",
        "            \"subsample\": 0.800800019181945,\n",
        "            \"colsample_bylevel\": 0.7264139639244361,\n",
        "            \"colsample_bytree\": 0.7283285945816331,\n",
        "            \"reg_alpha\": 0.13673920290025274,\n",
        "            \"reg_lambda\": 0.008614256283329808,\n",
        "        }\n",
        "        model2 = XGBRegressor(**params)\n",
        "\n",
        "        # モデル3\n",
        "        params = {\"n_estimators\": 56, \"random_state\": 13, \"verbose\": -1}\n",
        "        model3 = LGBMRegressor(**params)\n",
        "\n",
        "        # モデル1の学習と予測\n",
        "        _ = model1.fit(X_train, np.log1p(y_train))\n",
        "        valid_preds1 = np.expm1(model1.predict(X_valid))\n",
        "        rmsle1 = root_mean_squared_log_error(y_valid, valid_preds1)\n",
        "        print(f\"Fold {fold+1} RMSLE for model1 = {rmsle1}\")\n",
        "\n",
        "        # モデル2の学習と予測\n",
        "        _ = model2.fit(X_train, np.log1p(y_train))\n",
        "        valid_preds2 = np.expm1(model2.predict(X_valid))\n",
        "        rmsle2 = root_mean_squared_log_error(y_valid, valid_preds2)\n",
        "        print(f\"Fold {fold+1} RMSLE for model2 = {rmsle2}\")\n",
        "\n",
        "        # モデル3の学習と予測\n",
        "        _ = model3.fit(X_train, np.log1p(y_train))\n",
        "        valid_preds3 = np.expm1(model3.predict(X_valid))\n",
        "        rmsle3 = root_mean_squared_log_error(y_valid, valid_preds3)\n",
        "        print(f\"Fold {fold+1} RMSLE for model3 = {rmsle3}\")\n",
        "\n",
        "        # 損失とモデルの記録\n",
        "        loss_dict[\"model1\"] = rmsle1\n",
        "        loss_dict[\"model2\"] = rmsle2\n",
        "        loss_dict[\"model3\"] = rmsle3\n",
        "\n",
        "        model_dict[\"model1\"] = model1\n",
        "        model_dict[\"model2\"] = model2\n",
        "        model_dict[\"model3\"] = model3\n",
        "\n",
        "        valid_pred_dict[\"model1\"] = valid_preds1\n",
        "        valid_pred_dict[\"model2\"] = valid_preds2\n",
        "        valid_pred_dict[\"model3\"] = valid_preds3\n",
        "\n",
        "        # 平均予測\n",
        "        valid_preds_mean = np.mean(list(valid_pred_dict.values()), axis=0)\n",
        "        rmsle_mean = root_mean_squared_log_error(y_valid, valid_preds_mean)\n",
        "        print(f\"Fold {fold+1} Average RMSLE = {rmsle_mean}\")\n",
        "\n",
        "        min_loss_model = min(loss_dict, key=loss_dict.get)\n",
        "\n",
        "        model = model_dict[min_loss_model]\n",
        "        valid_preds_best_model = np.expm1(model.predict(X_valid))\n",
        "        rmsle_best = root_mean_squared_log_error(y_valid, valid_preds_best_model)\n",
        "\n",
        "        if rmsle_mean > rmsle_best:\n",
        "            print(f\"The average RMSLE is {rmsle_mean} while the best RMSLE is {rmsle_best}. Using the best model.\")\n",
        "            oof_predictions[valid_index] = valid_preds_best_model\n",
        "        else:\n",
        "            print(f\"The average RMSLE is {rmsle_mean} while the best RMSLE is {rmsle_best}. Averaging all models.\")\n",
        "            oof_predictions[valid_index] = valid_preds_mean\n",
        "\n",
        "        # 全Trainデータ予測\n",
        "        for train_idx in train_index:\n",
        "            train_predictions[train_idx] += valid_preds_best_model.mean() / kf.n_splits\n",
        "\n",
        "        # テストデータの予測\n",
        "        test_preds = model.predict(test)\n",
        "        test_predictions += test_preds / kf.n_splits\n",
        "\n",
        "        gc.collect()\n",
        "        print(\"---------------\\n\")\n",
        "\n",
        "    RMSLE = root_mean_squared_log_error(target, oof_predictions)\n",
        "    print(f\"OOF RMSLE = {RMSLE}\")\n",
        "    print(f\"Train RMSLE = {root_mean_squared_log_error(target, train_predictions)}\")\n",
        "\n",
        "    return oof_predictions, train_predictions, np.expm1(test_predictions)\n"
      ],
      "metadata": {
        "id": "DApbCia_85Gd"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# oof_predictions,test_predictions = get_models_trained(train,test,target,30)\n",
        "oof_predictions, train_predictions, test_predictions = get_models_trained(train, test, target, 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1bRoEIUcVeN",
        "outputId": "dfdf0784-1dcc-44c4-b9ec-b18342c1c680"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------ fold 1 ------------------------------\n",
            "Fold 1 RMSLE for model1 = 0.7415377868454196\n",
            "Fold 1 RMSLE for model2 = 0.7401615326048081\n",
            "Fold 1 RMSLE for model3 = 0.7588276094861218\n",
            "Fold 1 Average RMSLE = 0.7435481212060641\n",
            "The average RMSLE is 0.7435481212060641 while the best RMSLE is 0.7401615326048081. Using the best model.\n",
            "---------------\n",
            "\n",
            "------------------------------ fold 2 ------------------------------\n",
            "Fold 2 RMSLE for model1 = 0.6208200937805237\n",
            "Fold 2 RMSLE for model2 = 0.6170278922675616\n",
            "Fold 2 RMSLE for model3 = 0.6456541275864296\n",
            "Fold 2 Average RMSLE = 0.6232742412669424\n",
            "The average RMSLE is 0.6232742412669424 while the best RMSLE is 0.6170278922675616. Using the best model.\n",
            "---------------\n",
            "\n",
            "------------------------------ fold 3 ------------------------------\n",
            "Fold 3 RMSLE for model1 = 0.7557214519060115\n",
            "Fold 3 RMSLE for model2 = 0.7562952177184386\n",
            "Fold 3 RMSLE for model3 = 0.7637242061317118\n",
            "Fold 3 Average RMSLE = 0.7550316082904608\n",
            "The average RMSLE is 0.7550316082904608 while the best RMSLE is 0.7557214519060115. Averaging all models.\n",
            "---------------\n",
            "\n",
            "------------------------------ fold 4 ------------------------------\n",
            "Fold 4 RMSLE for model1 = 0.8125247377810508\n",
            "Fold 4 RMSLE for model2 = 0.8105252367470979\n",
            "Fold 4 RMSLE for model3 = 0.8220738065925356\n",
            "Fold 4 Average RMSLE = 0.8113561054903514\n",
            "The average RMSLE is 0.8113561054903514 while the best RMSLE is 0.8105252367470979. Using the best model.\n",
            "---------------\n",
            "\n",
            "------------------------------ fold 5 ------------------------------\n",
            "Fold 5 RMSLE for model1 = 0.7399117424349534\n",
            "Fold 5 RMSLE for model2 = 0.7364664532814172\n",
            "Fold 5 RMSLE for model3 = 0.7338498639033229\n",
            "Fold 5 Average RMSLE = 0.7327222527820986\n",
            "The average RMSLE is 0.7327222527820986 while the best RMSLE is 0.7338498639033229. Averaging all models.\n",
            "---------------\n",
            "\n",
            "------------------------------ fold 6 ------------------------------\n",
            "Fold 6 RMSLE for model1 = 0.7797581493369145\n",
            "Fold 6 RMSLE for model2 = 0.7822032385187814\n",
            "Fold 6 RMSLE for model3 = 0.801736547636133\n",
            "Fold 6 Average RMSLE = 0.7847671177734777\n",
            "The average RMSLE is 0.7847671177734777 while the best RMSLE is 0.7797581493369145. Using the best model.\n",
            "---------------\n",
            "\n",
            "------------------------------ fold 7 ------------------------------\n",
            "Fold 7 RMSLE for model1 = 0.8256517248239129\n",
            "Fold 7 RMSLE for model2 = 0.8285770595167614\n",
            "Fold 7 RMSLE for model3 = 0.8085419113450952\n",
            "Fold 7 Average RMSLE = 0.8167645136111108\n",
            "The average RMSLE is 0.8167645136111108 while the best RMSLE is 0.8085419113450952. Using the best model.\n",
            "---------------\n",
            "\n",
            "------------------------------ fold 8 ------------------------------\n",
            "Fold 8 RMSLE for model1 = 0.7980707069391493\n",
            "Fold 8 RMSLE for model2 = 0.7958859436991286\n",
            "Fold 8 RMSLE for model3 = 0.8310060634232151\n",
            "Fold 8 Average RMSLE = 0.8049164419294552\n",
            "The average RMSLE is 0.8049164419294552 while the best RMSLE is 0.7958859436991286. Using the best model.\n",
            "---------------\n",
            "\n",
            "------------------------------ fold 9 ------------------------------\n",
            "Fold 9 RMSLE for model1 = 0.7458080431275828\n",
            "Fold 9 RMSLE for model2 = 0.743733715416714\n",
            "Fold 9 RMSLE for model3 = 0.7363166312339574\n",
            "Fold 9 Average RMSLE = 0.7369385449944915\n",
            "The average RMSLE is 0.7369385449944915 while the best RMSLE is 0.7363166312339574. Using the best model.\n",
            "---------------\n",
            "\n",
            "------------------------------ fold 10 ------------------------------\n",
            "Fold 10 RMSLE for model1 = 0.7449756356374213\n",
            "Fold 10 RMSLE for model2 = 0.7486022890168358\n",
            "Fold 10 RMSLE for model3 = 0.752968874027899\n",
            "Fold 10 Average RMSLE = 0.7444282168700702\n",
            "The average RMSLE is 0.7444282168700702 while the best RMSLE is 0.7449756356374213. Averaging all models.\n",
            "---------------\n",
            "\n",
            "------------------------------ fold 11 ------------------------------\n",
            "Fold 11 RMSLE for model1 = 0.7025415078201295\n",
            "Fold 11 RMSLE for model2 = 0.7046850820984935\n",
            "Fold 11 RMSLE for model3 = 0.716733061190062\n",
            "Fold 11 Average RMSLE = 0.7026432672874442\n",
            "The average RMSLE is 0.7026432672874442 while the best RMSLE is 0.7025415078201295. Using the best model.\n",
            "---------------\n",
            "\n",
            "------------------------------ fold 12 ------------------------------\n",
            "Fold 12 RMSLE for model1 = 0.7534303404165359\n",
            "Fold 12 RMSLE for model2 = 0.752768245110899\n",
            "Fold 12 RMSLE for model3 = 0.7756753877495369\n",
            "Fold 12 Average RMSLE = 0.7578418169666628\n",
            "The average RMSLE is 0.7578418169666628 while the best RMSLE is 0.752768245110899. Using the best model.\n",
            "---------------\n",
            "\n",
            "------------------------------ fold 13 ------------------------------\n",
            "Fold 13 RMSLE for model1 = 0.7795699298275037\n",
            "Fold 13 RMSLE for model2 = 0.77716752086071\n",
            "Fold 13 RMSLE for model3 = 0.7982412758002728\n",
            "Fold 13 Average RMSLE = 0.7828083139840138\n",
            "The average RMSLE is 0.7828083139840138 while the best RMSLE is 0.77716752086071. Using the best model.\n",
            "---------------\n",
            "\n",
            "------------------------------ fold 14 ------------------------------\n",
            "Fold 14 RMSLE for model1 = 0.7771220120187421\n",
            "Fold 14 RMSLE for model2 = 0.7776917450521134\n",
            "Fold 14 RMSLE for model3 = 0.7826820549761381\n",
            "Fold 14 Average RMSLE = 0.7756489978041058\n",
            "The average RMSLE is 0.7756489978041058 while the best RMSLE is 0.7771220120187421. Averaging all models.\n",
            "---------------\n",
            "\n",
            "------------------------------ fold 15 ------------------------------\n",
            "Fold 15 RMSLE for model1 = 0.7146877562413658\n",
            "Fold 15 RMSLE for model2 = 0.7190053280998365\n",
            "Fold 15 RMSLE for model3 = 0.737497860756798\n",
            "Fold 15 Average RMSLE = 0.7199976442481798\n",
            "The average RMSLE is 0.7199976442481798 while the best RMSLE is 0.7146877562413658. Using the best model.\n",
            "---------------\n",
            "\n",
            "------------------------------ fold 16 ------------------------------\n",
            "Fold 16 RMSLE for model1 = 0.7729647943556965\n",
            "Fold 16 RMSLE for model2 = 0.7754331600409907\n",
            "Fold 16 RMSLE for model3 = 0.7837633681214972\n",
            "Fold 16 Average RMSLE = 0.773818718095174\n",
            "The average RMSLE is 0.773818718095174 while the best RMSLE is 0.7729647943556965. Using the best model.\n",
            "---------------\n",
            "\n",
            "------------------------------ fold 17 ------------------------------\n",
            "Fold 17 RMSLE for model1 = 0.794184218248587\n",
            "Fold 17 RMSLE for model2 = 0.7938063156157217\n",
            "Fold 17 RMSLE for model3 = 0.8219864874476299\n",
            "Fold 17 Average RMSLE = 0.799857035384445\n",
            "The average RMSLE is 0.799857035384445 while the best RMSLE is 0.7938063156157217. Using the best model.\n",
            "---------------\n",
            "\n",
            "------------------------------ fold 18 ------------------------------\n",
            "Fold 18 RMSLE for model1 = 0.7305731553216647\n",
            "Fold 18 RMSLE for model2 = 0.7285077248153737\n",
            "Fold 18 RMSLE for model3 = 0.7325768904477377\n",
            "Fold 18 Average RMSLE = 0.7272717473754239\n",
            "The average RMSLE is 0.7272717473754239 while the best RMSLE is 0.7285077248153737. Averaging all models.\n",
            "---------------\n",
            "\n",
            "------------------------------ fold 19 ------------------------------\n",
            "Fold 19 RMSLE for model1 = 0.7239894727277147\n",
            "Fold 19 RMSLE for model2 = 0.7180813762999969\n",
            "Fold 19 RMSLE for model3 = 0.7383763725595692\n",
            "Fold 19 Average RMSLE = 0.7208117528040645\n",
            "The average RMSLE is 0.7208117528040645 while the best RMSLE is 0.7180813762999969. Using the best model.\n",
            "---------------\n",
            "\n",
            "------------------------------ fold 20 ------------------------------\n",
            "Fold 20 RMSLE for model1 = 0.7962937583690636\n",
            "Fold 20 RMSLE for model2 = 0.7993374481035365\n",
            "Fold 20 RMSLE for model3 = 0.8075447647851189\n",
            "Fold 20 Average RMSLE = 0.7957818196006848\n",
            "The average RMSLE is 0.7957818196006848 while the best RMSLE is 0.7962937583690636. Averaging all models.\n",
            "---------------\n",
            "\n",
            "OOF RMSLE = 0.7538496920588333\n",
            "Train RMSLE = 1.4873064894859278\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submit  =  pd . read_csv ( 'sample_submission.csv' ,  header = None )\n",
        "submit [ 1 ]  =  test_predictions\n",
        "submit . to_csv ( 'submission_26.csv' ,  header = None ,  index = False )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "7zJEoCYjciqd",
        "outputId": "63ba7697-c6d7-448d-c5f2-066336fbe23d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'sample_submission.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-75384b11f0ad>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msubmit\u001b[0m  \u001b[0;34m=\u001b[0m  \u001b[0mpd\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0mread_csv\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0;34m'sample_submission.csv'\u001b[0m \u001b[0;34m,\u001b[0m  \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msubmit\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m]\u001b[0m  \u001b[0;34m=\u001b[0m  \u001b[0mtest_predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msubmit\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0mto_csv\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0;34m'submission_26.csv'\u001b[0m \u001b[0;34m,\u001b[0m  \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m,\u001b[0m  \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sample_submission.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4D3tywJgWHwR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}