{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "12SvoeBjdeuZncWgMeLTyaopVNb59jwZ9",
      "authorship_tag": "ABX9TyNcrU8WXDvE9Xv3Te3dLpp6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skyshine460/GHG_Direct_Emissions/blob/main/submit_ghg16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install pygeohash"
      ],
      "metadata": {
        "id": "nAAvY8SydMBB"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install catboost"
      ],
      "metadata": {
        "id": "_ZCfJUYDdTpc"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "1fkDm39xbLdm"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import joblib\n",
        "import math\n",
        "\n",
        "import pygeohash as pgh\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import *\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mj0rnGI7dBPO",
        "outputId": "27d999ad-8e1c-4dae-e17a-17cc50d015d7"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 予測モデルを訓練するためのデータセット\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/GHGの排出量の予測にチャレンジしよう！（SMBC Group GREEN×DATA Challenge 2024）/提供データ/train.csv', index_col=0)\n",
        "\n",
        "# 予測モデルに推論（予測)させるデータセット\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/GHGの排出量の予測にチャレンジしよう！（SMBC Group GREEN×DATA Challenge 2024）/提供データ/test.csv', index_col=0)"
      ],
      "metadata": {
        "id": "3yD9DfwbcwFe"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# del train_df['Unnamed: 0'], test_df['Unnamed: 0']\n",
        "del train_df['FacilityName'], test_df['FacilityName']\n",
        "del train_df['LocationAddress'], test_df['LocationAddress']\n",
        "del train_df['ZIP'], test_df['ZIP']\n",
        "del train_df['IndustryType'], test_df['IndustryType']\n",
        "del train_df['SecondPrimaryNAICS'], test_df['SecondPrimaryNAICS']"
      ],
      "metadata": {
        "id": "aepPvmt4bP6k"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "two_digit_map     = {11: 'Agriculture, Forestry, Fishing and Hunting',\n",
        "                    21: 'Mining, Quarrying, and Oil and Gas Extraction',\n",
        "                    22: 'Utilities',\n",
        "                    23: 'Construction',\n",
        "                    31: 'Manufacturing',\n",
        "                    32: 'Manufacturing',\n",
        "                    33: 'Manufacturing',\n",
        "                    42: 'Wholesale Trade',\n",
        "                    44: 'Retail Trade',\n",
        "                    45: 'Retail Trade',\n",
        "                    48: 'Transportation and Warehousing',\n",
        "                    49: 'Transportation and Warehousing',\n",
        "                    51: 'Information',\n",
        "                    52: 'Finance and Insurance',\n",
        "                    53: 'Real Estate and Rental and Leasing',\n",
        "                    54: 'Professional, Scientific, and Technical Services',\n",
        "                    55: 'Management of Companies and Enterprises',\n",
        "                    56: 'Administrative and Support and Waste Management and Remediation Services',\n",
        "                    61: 'Educational Services',\n",
        "                    62: 'Health Care and Social Assistance',\n",
        "                    71: 'Arts, Entertainment, and Recreation',\n",
        "                    72: 'Accommodation and Food Services',\n",
        "                    81: 'Other Services (except Public Administration)',\n",
        "                    92: 'Public Administration'}"
      ],
      "metadata": {
        "id": "-KsnfJvKbTE4"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['first_two_digit_primary_naics'] = train_df['PrimaryNAICS'].apply(lambda z: str(z)[:2]).astype(int)\n",
        "test_df['first_two_digit_primary_naics']  = test_df['PrimaryNAICS'].apply(lambda z: str(z)[:2]).astype(int)\n",
        "\n",
        "train_df['Economic_Sector']               = train_df['first_two_digit_primary_naics'].map(two_digit_map)\n",
        "test_df['Economic_Sector']                = test_df['first_two_digit_primary_naics'].map(two_digit_map)\n",
        "\n",
        "del train_df['first_two_digit_primary_naics'], test_df['first_two_digit_primary_naics']\n",
        "\n",
        "econ_sector_train                         = train_df['Economic_Sector'].values\n",
        "econ_sector_test                          = test_df['Economic_Sector'].values"
      ],
      "metadata": {
        "id": "CKG9fp8PbWJn"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_distance = pd.read_csv('/content/drive/MyDrive/GHGの排出量の予測にチャレンジしよう！（SMBC Group GREEN×DATA Challenge 2024）/特徴量/train_distance_5nbrs_.csv')\n",
        "test_distance = pd.read_csv('/content/drive/MyDrive/GHGの排出量の予測にチャレンジしよう！（SMBC Group GREEN×DATA Challenge 2024）/特徴量/test_distance_5nbrs_.csv')"
      ],
      "metadata": {
        "id": "_hLn0b4M9N76"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def haversine(lat1, lon1, lat2, lon2):\n",
        "#     # Convert latitude and longitude from degrees to radians\n",
        "#     lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
        "\n",
        "#     # Haversine formula\n",
        "#     dlat = lat2 - lat1\n",
        "#     dlon = lon2 - lon1\n",
        "#     a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n",
        "#     c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
        "\n",
        "#     # Radius of Earth in kilometers (mean radius)\n",
        "#     R = 6371.0\n",
        "\n",
        "#     # Calculate the distance\n",
        "#     distance = R * c\n",
        "#     return distance\n"
      ],
      "metadata": {
        "id": "mB1IM76_bb0p"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def calculate_distance(lat1, lon1, lat2, lon2):\n",
        "#     return haversine(lat1, lon1, lat2, lon2)\n",
        "\n",
        "# def get_nearest_distance(lat1, lon1, econ_sector_ref, neighbours=5, train_point=False):\n",
        "#     # Extract the required columns once to NumPy arrays\n",
        "#     latitudes = train_df['Latitude'].values\n",
        "#     longitudes = train_df['Longitude'].values\n",
        "#     ghg_emissions = train_df['GHG_Direct_Emissions_14_in_metric_tons'].values\n",
        "#     econ_sectors = train_df['Economic_Sector'].values\n",
        "\n",
        "#     # Filter NaN emissions directly\n",
        "#     valid_indices = ~np.isnan(ghg_emissions)\n",
        "\n",
        "#     latitudes = latitudes[valid_indices]\n",
        "#     longitudes = longitudes[valid_indices]\n",
        "#     ghg_emissions = ghg_emissions[valid_indices]\n",
        "#     econ_sectors = econ_sectors[valid_indices]\n",
        "\n",
        "#     # Parallelize distance calculation using ThreadPoolExecutor\n",
        "#     with ThreadPoolExecutor() as executor:\n",
        "#         distances = list(executor.map(calculate_distance,\n",
        "#                                      [lat1]*len(latitudes),\n",
        "#                                      [lon1]*len(longitudes),\n",
        "#                                      latitudes,\n",
        "#                                      longitudes))\n",
        "\n",
        "#     # Combine the results into a DataFrame\n",
        "#     near_df = pd.DataFrame({\n",
        "#         'Distance': distances,\n",
        "#         'GHG_emission_14': ghg_emissions,\n",
        "#         'Economic_Sector': econ_sectors\n",
        "#     })\n",
        "\n",
        "\n",
        "#     if train_point:\n",
        "#         near_df.sort_values(by='Distance', inplace=True)\n",
        "#         near_df = near_df.dropna()\n",
        "#         near_df = near_df.iloc[1:].reset_index(drop=True)\n",
        "#     else:\n",
        "#         # Sort distances by 'Distance'\n",
        "#         near_df.sort_values(by='Distance', inplace=True)\n",
        "#         near_df = near_df.dropna()\n",
        "\n",
        "#     # Filter by economic sector\n",
        "#     nearest_locations_econ_sector = near_df[near_df['Economic_Sector'] == econ_sector_ref]\n",
        "\n",
        "#     # Get the top N nearest for both economic sector and overall\n",
        "#     sub_near_econ = nearest_locations_econ_sector.head(neighbours)\n",
        "#     sub_nearest_locations = near_df.head(neighbours)\n",
        "\n",
        "#     # Compute weighted averages\n",
        "#     econ_weighted_average = (sub_near_econ['GHG_emission_14'] / np.where(sub_near_econ['Distance'] == 0, 1, sub_near_econ['Distance']) ).sum()\n",
        "#     near_weighted_average = (sub_nearest_locations['GHG_emission_14'] / np.where(sub_nearest_locations['Distance']==0,1,sub_nearest_locations['Distance'])).sum()\n",
        "\n",
        "#     # Compute regular averages\n",
        "#     econ_average = sub_near_econ['GHG_emission_14'].mean()\n",
        "#     near_average = sub_nearest_locations['GHG_emission_14'].mean()\n",
        "\n",
        "#     return [econ_weighted_average, econ_average, near_weighted_average, near_average]"
      ],
      "metadata": {
        "id": "gKmXQV4objgW"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# neighbours = 5\n",
        "\n",
        "# file_path = f\"test_distance_{neighbours}nbrs.csv\"\n",
        "# if os.path.exists(file_path):\n",
        "#     print(f\"{file_path} exists. \")\n",
        "#     test_distance = pd.read_csv(f\"test_distance_{neighbours}nbrs.csv\")\n",
        "# else:\n",
        "#     output = []\n",
        "#     for index in tqdm(range(test_df.shape[0])):\n",
        "\n",
        "#         lat1 = test_df.iloc[index]['Latitude']\n",
        "#         lon1 = test_df.iloc[index]['Longitude']\n",
        "#         econ_sector_ref = test_df.iloc[index]['Economic_Sector']\n",
        "#         x = get_nearest_distance(lat1,lon1,econ_sector_ref,neighbours=neighbours,train_point=False)\n",
        "#         output.append(x)\n",
        "\n",
        "#     test_distance = pd.DataFrame(output,columns = ['Economy_Sector_Weighted_Avg','Economic_Sector_Average','Nearest_Weighted_Average','Nearest_Average'])\n",
        "\n",
        "#     test_distance.to_csv(f'test_distance_{neighbours}nbrs.csv',index=False)\n",
        "\n",
        "\n",
        "# file_path = f\"train_distance_{neighbours}nbrs.csv\"\n",
        "# if os.path.exists(file_path):\n",
        "#     print(f\"{file_path} exists. \")\n",
        "#     train_distance = pd.read_csv(f'train_distance_{neighbours}nbrs.csv')\n",
        "# else:\n",
        "#     output = []\n",
        "#     for index in tqdm(range(train_df.shape[0])):\n",
        "#         lat1 = train_df.iloc[index]['Latitude']\n",
        "#         lon1 = train_df.iloc[index]['Longitude']\n",
        "#         econ_sector_ref = train_df.iloc[index]['Economic_Sector']\n",
        "#         x = get_nearest_distance(lat1,lon1,econ_sector_ref,neighbours=neighbours, train_point=True)\n",
        "#         output.append(x)\n",
        "\n",
        "#     train_distance = pd.DataFrame(output,columns = ['Economy_Sector_Weighted_Avg','Economic_Sector_Average','Nearest_Weighted_Average','Nearest_Average'])\n",
        "#     train_distance.to_csv(f'train_distance_{neighbours}nbrs.csv',index=False)\n",
        "\n",
        "\n",
        "\n",
        "train_df = pd.concat((train_df,train_distance),axis=1)\n",
        "\n",
        "# インデックスをリセットしてから結合\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "test_distance = test_distance.reset_index(drop=True)\n",
        "\n",
        "# 結合\n",
        "test_df = pd.concat((test_df, test_distance), axis=1)"
      ],
      "metadata": {
        "id": "fIiv6NWhbprU"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbeC-3jmPoIc",
        "outputId": "a11d5d99-0fe7-4677-e5a9-731f3967751c"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2508, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_features1(df):\n",
        "\n",
        "\n",
        "    # Year-over-year difference\n",
        "    df['TRI_Air_Emissions_YoY_Change_11'] = df['TRI_Air_Emissions_11_in_lbs'] - df['TRI_Air_Emissions_10_in_lbs']\n",
        "    df['TRI_Air_Emissions_YoY_Change_12'] = df['TRI_Air_Emissions_12_in_lbs'] - df['TRI_Air_Emissions_11_in_lbs']\n",
        "    df['TRI_Air_Emissions_YoY_Change_13'] = df['TRI_Air_Emissions_13_in_lbs'] - df['TRI_Air_Emissions_12_in_lbs']\n",
        "\n",
        "   # Year-over-year growth rate with NaN protection\n",
        "    df['TRI_Air_Emissions_Growth_Rate_11'] = np.where(\n",
        "        df['TRI_Air_Emissions_10_in_lbs'].notna() & (df['TRI_Air_Emissions_10_in_lbs'] != 0),\n",
        "        (df['TRI_Air_Emissions_11_in_lbs'] - df['TRI_Air_Emissions_10_in_lbs']) / df['TRI_Air_Emissions_10_in_lbs'],\n",
        "        np.nan\n",
        "    )\n",
        "\n",
        "    df['TRI_Air_Emissions_Growth_Rate_12'] = np.where(\n",
        "        df['TRI_Air_Emissions_11_in_lbs'].notna() & (df['TRI_Air_Emissions_11_in_lbs'] != 0),\n",
        "        (df['TRI_Air_Emissions_12_in_lbs'] - df['TRI_Air_Emissions_11_in_lbs']) / df['TRI_Air_Emissions_11_in_lbs'],\n",
        "        np.nan\n",
        "    )\n",
        "\n",
        "    df['TRI_Air_Emissions_Growth_Rate_13'] = np.where(\n",
        "        df['TRI_Air_Emissions_12_in_lbs'].notna() & (df['TRI_Air_Emissions_12_in_lbs'] != 0),\n",
        "        (df['TRI_Air_Emissions_13_in_lbs'] - df['TRI_Air_Emissions_12_in_lbs']) / df['TRI_Air_Emissions_12_in_lbs'],\n",
        "        np.nan\n",
        "    )\n",
        "\n",
        "    return df\n",
        "\n",
        "train_df      = create_features1(train_df)\n",
        "test_df       = create_features1(test_df)\n",
        "new_features1 = ['TRI_Air_Emissions_YoY_Change_11','TRI_Air_Emissions_YoY_Change_12','TRI_Air_Emissions_YoY_Change_13',\n",
        "                 'TRI_Air_Emissions_Growth_Rate_11','TRI_Air_Emissions_Growth_Rate_12','TRI_Air_Emissions_Growth_Rate_13']\n"
      ],
      "metadata": {
        "id": "DuD0maJEb1i5"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_features2(df):\n",
        "\n",
        "\n",
        "    # Year-over-year difference\n",
        "    df['GHG_Direct_Emissions_YoY_Change_11'] = df['GHG_Direct_Emissions_11_in_metric_tons'] - df['GHG_Direct_Emissions_10_in_metric_tons']\n",
        "    df['GHG_Direct_Emissions_YoY_Change_12'] = df['GHG_Direct_Emissions_12_in_metric_tons'] - df['GHG_Direct_Emissions_11_in_metric_tons']\n",
        "    df['GHG_Direct_Emissions_YoY_Change_13'] = df['GHG_Direct_Emissions_13_in_metric_tons'] - df['GHG_Direct_Emissions_12_in_metric_tons']\n",
        "\n",
        "    # Year-over-year growth rate for GHG Direct Emissions with NaN protection\n",
        "    df['GHG_Direct_Emissions_Growth_Rate_11'] = np.where(\n",
        "        df['GHG_Direct_Emissions_10_in_metric_tons'].notna() & (df['GHG_Direct_Emissions_10_in_metric_tons'] != 0),\n",
        "        (df['GHG_Direct_Emissions_11_in_metric_tons'] - df['GHG_Direct_Emissions_10_in_metric_tons']) / df['GHG_Direct_Emissions_10_in_metric_tons'],\n",
        "        np.nan\n",
        "    )\n",
        "\n",
        "    df['GHG_Direct_Emissions_Growth_Rate_12'] = np.where(\n",
        "        df['GHG_Direct_Emissions_11_in_metric_tons'].notna() & (df['GHG_Direct_Emissions_11_in_metric_tons'] != 0),\n",
        "        (df['GHG_Direct_Emissions_12_in_metric_tons'] - df['GHG_Direct_Emissions_11_in_metric_tons']) / df['GHG_Direct_Emissions_11_in_metric_tons'],\n",
        "        np.nan\n",
        "    )\n",
        "\n",
        "    df['GHG_Direct_Emissions_Growth_Rate_13'] = np.where(\n",
        "        df['GHG_Direct_Emissions_12_in_metric_tons'].notna() & (df['GHG_Direct_Emissions_12_in_metric_tons'] != 0),\n",
        "        (df['GHG_Direct_Emissions_13_in_metric_tons'] - df['GHG_Direct_Emissions_12_in_metric_tons']) / df['GHG_Direct_Emissions_12_in_metric_tons'],\n",
        "        np.nan\n",
        "    )\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "new_features2 = ['GHG_Direct_Emissions_YoY_Change_11','GHG_Direct_Emissions_YoY_Change_12','GHG_Direct_Emissions_YoY_Change_13',\n",
        "                 'GHG_Direct_Emissions_Growth_Rate_11','GHG_Direct_Emissions_Growth_Rate_12','GHG_Direct_Emissions_Growth_Rate_13'\n",
        "                 ]\n",
        "train_df      = create_features2(train_df)\n",
        "test_df       = create_features2(test_df)"
      ],
      "metadata": {
        "id": "2zv0CSwKb7iK"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_features3(df):\n",
        "    df['TRI_to_GHG_Ratio_10'] = np.where(df['GHG_Direct_Emissions_10_in_metric_tons'].notna(),\n",
        "                                     df['TRI_Air_Emissions_10_in_lbs'] / df['GHG_Direct_Emissions_10_in_metric_tons'],\n",
        "                                     np.nan)\n",
        "\n",
        "    df['TRI_to_GHG_Ratio_11'] = np.where(df['GHG_Direct_Emissions_11_in_metric_tons'].notna(),\n",
        "                                        df['TRI_Air_Emissions_11_in_lbs'] / df['GHG_Direct_Emissions_11_in_metric_tons'],\n",
        "                                        np.nan)\n",
        "\n",
        "    df['TRI_to_GHG_Ratio_12'] = np.where(df['GHG_Direct_Emissions_12_in_metric_tons'].notna(),\n",
        "                                        df['TRI_Air_Emissions_12_in_lbs'] / df['GHG_Direct_Emissions_12_in_metric_tons'],\n",
        "                                        np.nan)\n",
        "\n",
        "    df['TRI_to_GHG_Ratio_13'] = np.where(df['GHG_Direct_Emissions_13_in_metric_tons'].notna(),\n",
        "                                        df['TRI_Air_Emissions_13_in_lbs'] / df['GHG_Direct_Emissions_13_in_metric_tons'],\n",
        "                                        np.nan)\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "train_df      = create_features3(train_df)\n",
        "test_df       = create_features3(test_df)\n",
        "new_features3 = ['TRI_to_GHG_Ratio_10','TRI_to_GHG_Ratio_11','TRI_to_GHG_Ratio_12','TRI_to_GHG_Ratio_13']"
      ],
      "metadata": {
        "id": "c8zMly0JcABw"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_df = train_df.groupby(['Economic_Sector', 'State']).agg({'GHG_Direct_Emissions_14_in_metric_tons': ['mean', 'median', 'max', 'min','count']})\n",
        "\n",
        "summary_df.columns = [\n",
        "                        'GHG_Direct_Emissions_14_in_metric_tons_mean',\n",
        "                        'GHG_Direct_Emissions_14_in_metric_tons_median',\n",
        "                        'GHG_Direct_Emissions_14_in_metric_tons_max',\n",
        "                        'GHG_Direct_Emissions_14_in_metric_tons_min',\n",
        "                        'GHG_Direct_Emissions_14_in_metric_tons_count'\n",
        "                     ]\n",
        "\n",
        "summary_df = summary_df.reset_index()\n",
        "\n",
        "train_df = train_df.merge(summary_df, on=['Economic_Sector', 'State'], how='left')\n",
        "test_df  = test_df.merge(summary_df,  on=['Economic_Sector', 'State'], how='left')\n"
      ],
      "metadata": {
        "id": "QheQjnL_cEv3"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = pd.concat((train_df,test_df),axis=0)\n",
        "for cols in ['City','State','County','FIPScode','PrimaryNAICS','Economic_Sector']:\n",
        "    le              = LabelEncoder()\n",
        "    merged_df[cols] = le.fit_transform(merged_df[cols].values.reshape(-1,1))\n",
        "\n",
        "train_df = merged_df.iloc[:train_df.shape[0],:]\n",
        "test_df  = merged_df.iloc[train_df.shape[0]:,:]\n",
        "train_df.shape,test_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcxLwwNvcHnD",
        "outputId": "b513d585-ce74-4f7d-8358-0b635c8b321c"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4655, 42), (2508, 42))"
            ]
          },
          "metadata": {},
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_columns = [\n",
        "                    'TRI_Air_Emissions_10_in_lbs', 'TRI_Air_Emissions_11_in_lbs',\n",
        "                    'TRI_Air_Emissions_12_in_lbs', 'TRI_Air_Emissions_13_in_lbs','PrimaryNAICS',\n",
        "                    'GHG_Direct_Emissions_10_in_metric_tons', 'GHG_Direct_Emissions_11_in_metric_tons',\n",
        "                    'GHG_Direct_Emissions_12_in_metric_tons', 'GHG_Direct_Emissions_13_in_metric_tons',\n",
        "                    ]\n",
        "lat_lon_columns   = ['Latitude','Longitude']\n",
        "target_columns    = ['GHG_Direct_Emissions_14_in_metric_tons']\n",
        "categorical_columns = ['City','State','County','FIPScode','PrimaryNAICS','Economic_Sector']\n",
        "train_aggregations  =  [\n",
        "                        'GHG_Direct_Emissions_14_in_metric_tons_mean',\n",
        "                        'GHG_Direct_Emissions_14_in_metric_tons_median',\n",
        "                        'GHG_Direct_Emissions_14_in_metric_tons_max',\n",
        "                        'GHG_Direct_Emissions_14_in_metric_tons_min',\n",
        "                        'GHG_Direct_Emissions_14_in_metric_tons_count'\n",
        "                     ]\n",
        "new_features1  = ['TRI_Air_Emissions_YoY_Change_11','TRI_Air_Emissions_YoY_Change_12','TRI_Air_Emissions_YoY_Change_13',\n",
        "                    'TRI_Air_Emissions_Growth_Rate_11','TRI_Air_Emissions_Growth_Rate_12','TRI_Air_Emissions_Growth_Rate_13']\n",
        "\n",
        "new_features2   = ['GHG_Direct_Emissions_YoY_Change_11','GHG_Direct_Emissions_YoY_Change_12','GHG_Direct_Emissions_YoY_Change_13',\n",
        "                  'GHG_Direct_Emissions_Growth_Rate_11','GHG_Direct_Emissions_Growth_Rate_12','GHG_Direct_Emissions_Growth_Rate_13'\n",
        "                  ]\n",
        "new_features3   = ['TRI_to_GHG_Ratio_10','TRI_to_GHG_Ratio_11','TRI_to_GHG_Ratio_12','TRI_to_GHG_Ratio_13']\n",
        "neighbour_feats = ['Economy_Sector_Weighted_Avg','Economic_Sector_Average','Nearest_Weighted_Average','Nearest_Average']\n",
        "\n",
        "\n",
        "train = train_df[numerical_columns+\n",
        "                 lat_lon_columns+\n",
        "                #  categorical_columns+\n",
        "                 new_features1+['Economic_Sector']\n",
        "                #  new_features2\n",
        "               #   new_features3+\n",
        "                #  train_aggregations+\n",
        "               #   neighbour_feats\n",
        "                 ].values\n",
        "test  = test_df[numerical_columns+\n",
        "                 lat_lon_columns+\n",
        "                #  categorical_columns+\n",
        "                 new_features1+['Economic_Sector']\n",
        "                #  new_features2\n",
        "               #   new_features3+\n",
        "                #  train_aggregations+\n",
        "               #   neighbour_feats\n",
        "                 ].values\n",
        "target = train_df[target_columns].values"
      ],
      "metadata": {
        "id": "oqUiXtVlcLwh"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcDy6cJXOwxz",
        "outputId": "b911d460-b81b-4c1c-c07a-44b9600c7ae5"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2508, 18)"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_models_trained(train,test,target, num_folds):\n",
        "#     kf               = KFold(n_splits=num_folds, shuffle=True, random_state=13)\n",
        "\n",
        "#     oof_predictions  = np.zeros(len(train))\n",
        "#     test_predictions = np.zeros(len(test))\n",
        "\n",
        "\n",
        "#     for fold, (train_index, valid_index) in enumerate(kf.split(train,target)):\n",
        "\n",
        "#         X_train, X_valid             = train[train_index], train[valid_index]\n",
        "#         y_train, y_valid             = target[train_index], target[valid_index]\n",
        "\n",
        "\n",
        "#         model_dict                   = {}\n",
        "#         loss_dict                    = {}\n",
        "#         global valid_pred_dict\n",
        "#         valid_pred_dict              = {}\n",
        "\n",
        "#         params                       = {\"n_estimators\": 626, \"max_depth\": 3, \"random_state\":13,\n",
        "#                                         \"min_child_weight\": 0.001190123543553736, \"learning_rate\": 0.010519736270936835,\n",
        "#                                         \"subsample\": 0.7304788478701394, \"colsample_bylevel\": 0.604447278915981,\n",
        "#                                           \"colsample_bytree\": 0.7616852136157319, \"reg_alpha\": 0.115175569924065, \"reg_lambda\": 0.07155347824929895}\n",
        "#         model1                       = XGBRegressor(**params)\n",
        "\n",
        "#         params                       = {\"n_estimators\": 206, \"max_depth\": 3, \"min_child_weight\": 0.002124665025111174, \"random_state\":13,\n",
        "#                                         \"learning_rate\": 0.02528390491004826, \"subsample\": 0.800800019181945,\n",
        "#                                         \"colsample_bylevel\": 0.7264139639244361, \"colsample_bytree\": 0.7283285945816331,\n",
        "#                                           \"reg_alpha\": 0.13673920290025274, \"reg_lambda\": 0.008614256283329808}\n",
        "#         model2                       = XGBRegressor(**params)\n",
        "\n",
        "#         params                       = {\"n_estimators\": 56,\"random_state\":13,\n",
        "#                                          \"verbose\":-1,}\n",
        "#         model3                       = LGBMRegressor(**params)\n",
        "\n",
        "\n",
        "\n",
        "#         _                             = model1.fit(X_train,np.log1p(y_train))\n",
        "#         valid_preds1                  = np.expm1(model1.predict(X_valid))\n",
        "#         rmsle1                        = root_mean_squared_log_error(y_valid, valid_preds1)\n",
        "#         print(f\"Fold {fold+1} RMSLE for model1 = {rmsle1}\")\n",
        "\n",
        "#         _                             = model2.fit(X_train,np.log1p(y_train))\n",
        "#         valid_preds2                  = np.expm1(model2.predict(X_valid))\n",
        "#         rmsle2                        = root_mean_squared_log_error(y_valid, valid_preds2)\n",
        "#         print(f\"Fold {fold+1} RMSLE for model2 = {rmsle2}\")\n",
        "\n",
        "#         _                             = model3.fit(X_train,np.log1p(y_train))\n",
        "#         valid_preds3                  = np.expm1(model3.predict(X_valid))\n",
        "#         rmsle3                        = root_mean_squared_log_error(y_valid, valid_preds3)\n",
        "#         print(f\"Fold {fold+1} RMSLE for model3 = {rmsle3}\")\n",
        "\n",
        "\n",
        "\n",
        "#         loss_dict['model1']           = rmsle1\n",
        "#         loss_dict['model2']           = rmsle2\n",
        "#         loss_dict['model3']           = rmsle3\n",
        "\n",
        "\n",
        "#         model_dict['model1']          = model1\n",
        "#         model_dict['model2']          = model2\n",
        "#         model_dict['model3']          = model3\n",
        "\n",
        "\n",
        "#         valid_pred_dict['model1']     = valid_preds1\n",
        "#         valid_pred_dict['model2']     = valid_preds2\n",
        "#         valid_pred_dict['model3']     = valid_preds3\n",
        "\n",
        "\n",
        "\n",
        "#         valid_preds_mean              = np.mean(list(valid_pred_dict.values()), axis=0)\n",
        "#         rmsle_mean                    = root_mean_squared_log_error(y_valid, valid_preds_mean)\n",
        "#         print(f\"Fold {fold+1} Average RMSLE = {rmsle_mean}\")\n",
        "\n",
        "\n",
        "#         min_loss_model                = min(loss_dict, key=loss_dict.get)\n",
        "\n",
        "#         model                         = model_dict[min_loss_model]\n",
        "#         valid_preds_best_model        = np.expm1(model.predict(X_valid))\n",
        "#         rmsle_best                    = root_mean_squared_log_error(y_valid, valid_preds_best_model)\n",
        "\n",
        "\n",
        "#         if rmsle_mean >rmsle_best:\n",
        "#             print(f\"The average RMSLE is {rmsle_mean} while the best RMSLE is {rmsle_best} and we proceed with the model with best RMSLE\")\n",
        "#             model = model_dict[min_loss_model]\n",
        "#             oof_predictions[valid_index]  = valid_preds_best_model\n",
        "#             test_preds                    = model.predict(test)\n",
        "#             test_predictions += (test_preds) / kf.n_splits\n",
        "#             oof_predictions[valid_index]  = valid_preds_best_model\n",
        "\n",
        "\n",
        "#         else:\n",
        "#             print(f\"The average RMSLE is {rmsle_mean} while the best RMSLE is {rmsle_best} and we proceed with averaging of all models\")\n",
        "#             output_predictions = []\n",
        "#             for key,values in model_dict.items():\n",
        "#                 model       = values\n",
        "#                 test_preds  = model.predict(test)\n",
        "#                 _           = output_predictions.append(test_preds)\n",
        "#             output_preds    = np.mean(output_predictions, axis=0)\n",
        "#             test_predictions += output_preds/kf.n_splits\n",
        "#             oof_predictions[valid_index]  = valid_preds_mean\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#         gc.collect()\n",
        "\n",
        "#         print('---------------\\n')\n",
        "\n",
        "#     RMSLE = root_mean_squared_log_error(target, oof_predictions)\n",
        "#     print(f\"OOF RMSLE = {RMSLE}\")\n",
        "\n",
        "#     return oof_predictions,np.expm1(test_predictions)\n"
      ],
      "metadata": {
        "id": "NWRcxoFvcSmV"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import root_mean_squared_log_error\n",
        "from sklearn.model_selection import KFold\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from xgboost import XGBRegressor\n",
        "import gc\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "def get_models_trained(train, test, target, num_folds):\n",
        "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    oof_predictions = np.zeros(len(train))\n",
        "    test_predictions = np.zeros(len(test))\n",
        "    train_predictions = np.zeros(len(train))  # Trainデータ予測の格納用\n",
        "    fold_train_rmsle = []\n",
        "\n",
        "    for fold, (train_index, valid_index) in enumerate(kf.split(train, target)):\n",
        "        print(f\"------------------------------ fold {fold} ------------------------------\")\n",
        "\n",
        "        X_train, X_valid = train[train_index], train[valid_index]\n",
        "        y_train, y_valid = target[train_index], target[valid_index]\n",
        "\n",
        "        model_dict = {}\n",
        "        loss_dict = {}\n",
        "        valid_preds_dict = {}\n",
        "        train_preds_dict = {}\n",
        "        rmsle_dict = {}\n",
        "\n",
        "        # モデル1\n",
        "        params1 = {\n",
        "            \"n_estimators\": 500,\n",
        "            \"max_depth\": 4,\n",
        "            \"random_state\": 13,\n",
        "            \"min_child_weight\": 0.01,\n",
        "            \"learning_rate\": 0.01,\n",
        "            \"subsample\": 0.7,\n",
        "            \"colsample_bylevel\": 0.6,\n",
        "            \"colsample_bytree\": 0.7,\n",
        "            \"reg_alpha\": 0.1,\n",
        "            \"reg_lambda\": 10,\n",
        "        }\n",
        "        model1 = XGBRegressor(**params1)\n",
        "\n",
        "        # モデル1の学習と予測\n",
        "        _ = model1.fit(X_train, np.log1p(y_train))\n",
        "        valid_preds1 = np.expm1(model1.predict(X_valid))\n",
        "        train_preds1 = np.expm1(model1.predict(X_train))\n",
        "        rmsle1 = root_mean_squared_log_error(y_valid, valid_preds1)\n",
        "        tr_rmsle1 = root_mean_squared_log_error(y_train, train_preds1)\n",
        "        print(f\"Fold {fold} model1 tr:{tr_rmsle1:.5f} va:{rmsle1:.5f}\")\n",
        "\n",
        "        loss_dict[\"model1\"] = rmsle1\n",
        "        model_dict['model1'] = model1\n",
        "        valid_preds_dict['model1'] = valid_preds1\n",
        "        train_preds_dict['model1'] = train_preds1\n",
        "        rmsle_dict['model1'] = rmsle1\n",
        "\n",
        "        # モデル2\n",
        "        params2 = {\n",
        "            \"n_estimators\": 200,\n",
        "            \"max_depth\": 4,\n",
        "            \"min_child_weight\": 0.01,\n",
        "            \"random_state\": 42,\n",
        "            \"learning_rate\": 0.02,\n",
        "            \"subsample\": 0.7,\n",
        "            \"colsample_bylevel\": 0.7,\n",
        "            \"colsample_bytree\": 0.7,\n",
        "            \"reg_alpha\": 0.15,\n",
        "            \"reg_lambda\": 10,\n",
        "        }\n",
        "        model2 = XGBRegressor(**params2)\n",
        "\n",
        "        # モデル2の学習と予測\n",
        "        _ = model2.fit(X_train, np.log1p(y_train))\n",
        "        valid_preds2 = np.expm1(model2.predict(X_valid))\n",
        "        train_preds2 = np.expm1(model2.predict(X_train))\n",
        "        rmsle2 = root_mean_squared_log_error(y_valid, valid_preds2)\n",
        "        tr_rmsle2 = root_mean_squared_log_error(y_train, train_preds2)\n",
        "        print(f\"Fold {fold} model2 tr:{tr_rmsle2:.5f} va:{rmsle2:.5f}\")\n",
        "\n",
        "        loss_dict[\"model2\"] = rmsle2\n",
        "        model_dict['model2'] = model2\n",
        "        valid_preds_dict['model2'] = valid_preds2\n",
        "        train_preds_dict['model2'] = train_preds2\n",
        "        rmsle_dict['model2'] = rmsle2\n",
        "\n",
        "        # モデル3\n",
        "        params3 = {\n",
        "          'objective': 'regression',  # 回帰タスク\n",
        "          'metric': 'rmse',  # RMSEで評価\n",
        "          'boosting_type': 'gbdt',  # 勾配ブースティング木\n",
        "          'learning_rate': 0.1,  # 学習率\n",
        "          'verbose': -1,  # 詳細な出力を抑制\n",
        "          'random_state': 42,  # 乱数の固定\n",
        "          # 'lambda_l1': 0.1,  # L1正則項の強さ\n",
        "          'lambda_l2': 50,  # L2正則項の強さ\n",
        "          'max_depth': 4,  # 木の最大深さ\n",
        "          # \"min_data_in_leaf\": 50,\n",
        "          # 'num_leaves': 7,  # 最大リーフ数\n",
        "          'max_bin' : 100,\n",
        "          \"n_estimators\": 50\n",
        "        }\n",
        "        model3 = LGBMRegressor(**params3)\n",
        "\n",
        "        # モデル3の学習と予測\n",
        "        model3.fit(\n",
        "            X_train,\n",
        "            np.log1p(y_train),\n",
        "            eval_set=[(X_valid, y_valid)],\n",
        "            callbacks=[\n",
        "                lgb.early_stopping(stopping_rounds=50, verbose=False),\n",
        "                lgb.log_evaluation(0)],\n",
        "        )\n",
        "        valid_preds3 = np.expm1(model3.predict(X_valid))\n",
        "        train_preds3 = np.expm1(model3.predict(X_train))\n",
        "        rmsle3 = root_mean_squared_log_error(y_valid, valid_preds3)\n",
        "        tr_rmsle3 = root_mean_squared_log_error(y_train, train_preds3)\n",
        "        print(f\"Fold {fold} model3 tr:{tr_rmsle3:.5f} va:{rmsle3:.5f}\")\n",
        "\n",
        "        loss_dict[\"model3\"] = rmsle3\n",
        "        model_dict['model3'] = model3\n",
        "        valid_preds_dict['model3'] = valid_preds3\n",
        "        train_preds_dict['model3'] = train_preds3\n",
        "        rmsle_dict['model3'] = rmsle3\n",
        "\n",
        "\n",
        "        # 各モデルの予測の平均値を算出して評価\n",
        "        valid_preds_mean = np.mean(list(valid_preds_dict.values()), axis=0)\n",
        "        train_preds_mean = np.mean(list(train_preds_dict.values()), axis=0)\n",
        "\n",
        "        rmsle_valid_mean = root_mean_squared_log_error(y_valid, valid_preds_mean)\n",
        "        rmsle_train_mean = root_mean_squared_log_error(y_train, train_preds_mean)\n",
        "\n",
        "        rmsle_mean = root_mean_squared_log_error(y_valid, valid_preds_mean)\n",
        "        min_loss_model = min(loss_dict, key=loss_dict.get)\n",
        "        model = model_dict[min_loss_model]\n",
        "        valid_preds_best_model = np.expm1(model.predict(X_valid))\n",
        "        rmsle_best = root_mean_squared_log_error(y_valid, valid_preds_best_model)\n",
        "        train_preds_best_model = np.expm1(model.predict(X_train))\n",
        "        tr_rmsle_best = root_mean_squared_log_error(y_train, train_preds_best_model)\n",
        "\n",
        "        print(f\"Fold {fold} Ave    tr:{rmsle_train_mean:.5f} va:{rmsle_valid_mean:.5f}\")\n",
        "\n",
        "        # if rmsle_mean > rmsle_best:\n",
        "        #     print(f\"The average RMSLE is {rmsle_mean} while the best RMSLE is {rmsle_best}. Using the best model.\")\n",
        "        #     oof_predictions[valid_index] = valid_preds_best_model\n",
        "        #     train_predictions[train_index] = train_preds_best_model\n",
        "        # else:\n",
        "            # print(f\"The average RMSLE is {rmsle_mean} while the best RMSLE is {rmsle_best}. Averaging all models.\")\n",
        "        oof_predictions[valid_index] = valid_preds_mean\n",
        "        train_predictions[train_index] = train_preds_mean\n",
        "\n",
        "        # テストデータの予測\n",
        "        test_preds = model.predict(test)\n",
        "        test_predictions += test_preds / kf.n_splits\n",
        "\n",
        "        gc.collect()\n",
        "\n",
        "    RMSLE = root_mean_squared_log_error(target, oof_predictions)\n",
        "    tr_RMSLE = root_mean_squared_log_error(target, train_predictions)\n",
        "    print()\n",
        "    print(f\"[CV] tr:{tr_RMSLE:.5f} va:{RMSLE:.5f}\")\n",
        "\n",
        "    return oof_predictions, train_predictions, np.expm1(test_predictions)\n",
        "\n",
        "# oof_predictions,test_predictions = get_models_trained(train,test,target,30)\n",
        "oof_predictions, train_predictions, test_predictions = get_models_trained(train, test, target, 15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DApbCia_85Gd",
        "outputId": "3567df7a-c810-483b-b188-2272fce64164"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------ fold 0 ------------------------------\n",
            "Fold 0 model1 tr:0.71058 va:0.67995\n",
            "Fold 0 model2 tr:0.71788 va:0.67711\n",
            "Fold 0 model3 tr:0.72315 va:0.68326\n",
            "Fold 0 Ave    tr:0.71600 va:0.67895\n",
            "------------------------------ fold 1 ------------------------------\n",
            "Fold 1 model1 tr:0.70349 va:0.77705\n",
            "Fold 1 model2 tr:0.71211 va:0.78102\n",
            "Fold 1 model3 tr:0.71566 va:0.78707\n",
            "Fold 1 Ave    tr:0.70917 va:0.78032\n",
            "------------------------------ fold 2 ------------------------------\n",
            "Fold 2 model1 tr:0.70526 va:0.75887\n",
            "Fold 2 model2 tr:0.71407 va:0.75823\n",
            "Fold 2 model3 tr:0.71675 va:0.76263\n",
            "Fold 2 Ave    tr:0.71082 va:0.75859\n",
            "------------------------------ fold 3 ------------------------------\n",
            "Fold 3 model1 tr:0.70971 va:0.68843\n",
            "Fold 3 model2 tr:0.71746 va:0.68871\n",
            "Fold 3 model3 tr:0.72223 va:0.69050\n",
            "Fold 3 Ave    tr:0.71520 va:0.68794\n",
            "------------------------------ fold 4 ------------------------------\n",
            "Fold 4 model1 tr:0.70529 va:0.75815\n",
            "Fold 4 model2 tr:0.71327 va:0.76049\n",
            "Fold 4 model3 tr:0.71716 va:0.76787\n",
            "Fold 4 Ave    tr:0.71057 va:0.76097\n",
            "------------------------------ fold 5 ------------------------------\n",
            "Fold 5 model1 tr:0.70495 va:0.77356\n",
            "Fold 5 model2 tr:0.71239 va:0.77388\n",
            "Fold 5 model3 tr:0.71740 va:0.78332\n",
            "Fold 5 Ave    tr:0.71033 va:0.77534\n",
            "------------------------------ fold 6 ------------------------------\n",
            "Fold 6 model1 tr:0.70767 va:0.73425\n",
            "Fold 6 model2 tr:0.71512 va:0.73133\n",
            "Fold 6 model3 tr:0.71982 va:0.73689\n",
            "Fold 6 Ave    tr:0.71298 va:0.73291\n",
            "------------------------------ fold 7 ------------------------------\n",
            "Fold 7 model1 tr:0.70446 va:0.76799\n",
            "Fold 7 model2 tr:0.71327 va:0.77049\n",
            "Fold 7 model3 tr:0.71676 va:0.77488\n",
            "Fold 7 Ave    tr:0.71031 va:0.76993\n",
            "------------------------------ fold 8 ------------------------------\n",
            "Fold 8 model1 tr:0.70113 va:0.82696\n",
            "Fold 8 model2 tr:0.70828 va:0.82748\n",
            "Fold 8 model3 tr:0.71288 va:0.82611\n",
            "Fold 8 Ave    tr:0.70628 va:0.82580\n",
            "------------------------------ fold 9 ------------------------------\n",
            "Fold 9 model1 tr:0.70258 va:0.80251\n",
            "Fold 9 model2 tr:0.71129 va:0.80220\n",
            "Fold 9 model3 tr:0.71452 va:0.80645\n",
            "Fold 9 Ave    tr:0.70820 va:0.80230\n",
            "------------------------------ fold 10 ------------------------------\n",
            "Fold 10 model1 tr:0.70521 va:0.76486\n",
            "Fold 10 model2 tr:0.71322 va:0.76366\n",
            "Fold 10 model3 tr:0.71736 va:0.76119\n",
            "Fold 10 Ave    tr:0.71057 va:0.76206\n",
            "------------------------------ fold 11 ------------------------------\n",
            "Fold 11 model1 tr:0.70713 va:0.73904\n",
            "Fold 11 model2 tr:0.71459 va:0.73998\n",
            "Fold 11 model3 tr:0.71916 va:0.74566\n",
            "Fold 11 Ave    tr:0.71249 va:0.74025\n",
            "------------------------------ fold 12 ------------------------------\n",
            "Fold 12 model1 tr:0.70838 va:0.72311\n",
            "Fold 12 model2 tr:0.71637 va:0.72990\n",
            "Fold 12 model3 tr:0.71925 va:0.73952\n",
            "Fold 12 Ave    tr:0.71344 va:0.72886\n",
            "------------------------------ fold 13 ------------------------------\n",
            "Fold 13 model1 tr:0.70415 va:0.78484\n",
            "Fold 13 model2 tr:0.71182 va:0.78228\n",
            "Fold 13 model3 tr:0.71645 va:0.78179\n",
            "Fold 13 Ave    tr:0.70951 va:0.78182\n",
            "------------------------------ fold 14 ------------------------------\n",
            "Fold 14 model1 tr:0.70417 va:0.76205\n",
            "Fold 14 model2 tr:0.71267 va:0.76362\n",
            "Fold 14 model3 tr:0.71595 va:0.76100\n",
            "Fold 14 Ave    tr:0.70956 va:0.76073\n",
            "\n",
            "[CV] tr:0.71028 va:0.75735\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submit = pd.read_csv('/content/drive/MyDrive/GHGの排出量の予測にチャレンジしよう！（SMBC Group GREEN×DATA Challenge 2024）/提供データ/sample_submission.csv', header=None)\n",
        "submit[1] = test_predictions\n",
        "submit.to_csv('submit_ghg16.csv', header=None, index=False)\n",
        "\n",
        "# 投稿ファイルの中身を確認\n",
        "submit.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "7zJEoCYjciqd",
        "outputId": "1e38f983-32d2-452c-8c4e-498f5d476552"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      0              1\n",
              "0  4655   45619.158076\n",
              "1  4656  326654.480973\n",
              "2  4657   41066.725616\n",
              "3  4658   27356.854357\n",
              "4  4659   46870.237014"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6d4c702c-ff89-43d5-b907-f7f15242c65c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4655</td>\n",
              "      <td>45619.158076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4656</td>\n",
              "      <td>326654.480973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4657</td>\n",
              "      <td>41066.725616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4658</td>\n",
              "      <td>27356.854357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4659</td>\n",
              "      <td>46870.237014</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6d4c702c-ff89-43d5-b907-f7f15242c65c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6d4c702c-ff89-43d5-b907-f7f15242c65c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6d4c702c-ff89-43d5-b907-f7f15242c65c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a147b357-8075-40ae-8736-1402028106c6\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a147b357-8075-40ae-8736-1402028106c6')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a147b357-8075-40ae-8736-1402028106c6 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "submit",
              "summary": "{\n  \"name\": \"submit\",\n  \"rows\": 2508,\n  \"fields\": [\n    {\n      \"column\": 0,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 724,\n        \"min\": 4655,\n        \"max\": 7162,\n        \"num_unique_values\": 2508,\n        \"samples\": [\n          6776,\n          4711,\n          7135\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 1,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 320048.07311749284,\n        \"min\": 20148.19894882103,\n        \"max\": 1885985.001576829,\n        \"num_unique_values\": 2501,\n        \"samples\": [\n          31099.436981693587,\n          623643.5116921717,\n          26005.82996980781\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 171
        }
      ]
    }
  ]
}